# Language Models

- [LLM / transformer slides](https://docs.google.com/presentation/d/1uGS4w_UumzQAoq03S-sc9Nn7_hBaRNbQJf1G2SNXooU/edit?usp=sharing)

## Sequential Data and Recurrent Neural Networks

- ğŸ“š [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- ğŸ¨ [Four Experiments in Handwriting with a Neural Network](https://distill.pub/2016/handwriting/) (Drawing)
- ğŸ“– [10 things artificial intelligence did in 2018](http://aiweirdness.com/post/181621835642/10-things-artificial-intelligence-did-in-2018) by Janelle Shane (Text)
- ğŸ“– [Writing with the Machine](https://www.robinsloan.com/notes/writing-with-the-machine/)

## Transformers and Large Language Models

> among the reasons I use large pre-trained language models sparingly in my computer-generated poetry practice is that being able to know whose voices I'm speaking with is... actually important, as is being understanding how the output came to have its shape - [@aparrish](https://twitter.com/aparrish/), [full thread](https://twitter.com/aparrish/status/1286808606466244608)

- ğŸ“š [Watch an A.I. Learn to Write by Reading Nothing but **\_\_\_\_**](https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html) by Aatish Bhatia
- ğŸ“š [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Original "Transformer" paper from 2017, also [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) -- Attention paper from 2014
- ğŸ“š [What Are Transformer Models and How Do They Work?](https://docs.cohere.com/docs/transformer-models)
- ğŸ¥ [How large language models work, a visual intro to transformers](https://youtu.be/wjZofJX0v4M) by 3Blue1Brown
- ğŸ¥ [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g) by Andrej Karpathy and [Intro to LLMs slides](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view)
- ğŸ“– [Language Models Can Only Write Ransom Notes](https://posts.decontextualize.com/language-models-ransom-notes/) by Allison Parrish

## LLM Training

- ğŸ¦™ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- ğŸ¦™ [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)
- ğŸ¦œ [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œ](https://dl.acm.org/doi/10.1145/3442188.3445922)
- ğŸ” [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/May-2024/index.html)
- ğŸ“– [Generative AIâ€™s Illusory Case for Fair Use](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4924997) by Jacqueline Charlesworth

### Datasets for LLMs

- ğŸ”¢ [Common Crawl](https://commoncrawl.org/)
- ğŸ”¢ [The Pile](https://pile.eleuther.ai/)

### Climate Impact

- ğŸŒ [The Internetâ€™s Next Great Power Suck](https://www.theatlantic.com/technology/archive/2023/08/ai-carbon-emissions-data-centers/675094/)
- âš¡ï¸ [Carbon Emissions and Large Neural Network Training ](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)

## Code Examples and Implementations

### Replicate

_Examples will be shared over email due to use of ITP proxy server._

- ğŸ¨ Prompting Replicate text/image
- ğŸ’¬ ChatBot Conversations with Llama via Replicate. This follows the specification in the [Llama 3 Model Card](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/).

### Ollama

- ğŸ¦™ [Ollama: Run LLMs locally](https://ollama.ai/)
- ğŸ’» [Ollama with JavaScript](https://github.com/Programming-from-A-to-Z/Ollama-Examples)

### Transformers.js

- [Chatbot with Conversation History](https://editor.p5js.org/ima_ml/sketches/oGE7UEmwh)
- [Text Completion](https://editor.p5js.org/ima_ml/sketches/e9PFp5BkJ)

## Assignment

- Read [Language models can only write ransom notes](https://posts.decontextualize.com/language-models-ransom-notes/) by Allison Parrish and review the [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/May-2024/index.html). What questions arise for you about using LLMs in your work at ITP?
- Experiment with prompting a large language model in some way other than a provided interface (e.g. ChatGPT) and document the results in a blog post. Consider how working with an LLM compares to generating text from the other methods including but not limited to markov chains and context free grammars. Here are some options:
  - Run [any of the code examples above](#code-examples-and-implementations) Try adjusting the prompts, interaction, or visual design.
  - Try running Llama locally with [Ollama](https://ollama.ai/). Compare and contrast different models.

## Add your assignment below via Pull Request

_(Please note you are welcome to post under a pseudonym and/or password protect your published assignment. For NYU blogs, privacy options are covered in the [NYU Wordpress Knowledge Base](https://wp.nyu.edu/knowledge/). Finally, if you prefer not to post your assignment at all here, you may email the submission.)_
